Number of Training Datapoints : 79712
Number of Validation Datapoints : 4196

Training Logs

Step: 1
loss: 59.1176
grad_norm: 13818913.0
learning_rate: 0.001999732369864847
epoch: 0.0004014452027298274

Step: 100
loss: 0.5837
grad_norm: 450.974609375
learning_rate: 0.0019732369864846784
epoch: 0.04014452027298274

Step: 200
loss: 0.0025
grad_norm: 309.069580078125
learning_rate: 0.0019464739729693563
epoch: 0.08028904054596547

Step: 300
loss: 0.0022
grad_norm: 401.47113037109375
learning_rate: 0.0019197109594540345
epoch: 0.12043356081894821

Step: 400
loss: 0.0021
grad_norm: 163.6200408935547
learning_rate: 0.0018929479459387126
epoch: 0.16057808109193095

Step: 500
loss: 0.002
grad_norm: 327.5402526855469
learning_rate: 0.0018661849324233908
epoch: 0.20072260136491368

Step: 600
loss: 0.0019
grad_norm: 498.78369140625
learning_rate: 0.001839421918908069
epoch: 0.24086712163789642

Step: 700
loss: 0.002
grad_norm: 300.8548889160156
learning_rate: 0.0018126589053927473
epoch: 0.2810116419108792

Step: 800
loss: 0.0019
grad_norm: 342.8445129394531
learning_rate: 0.0017858958918774254
epoch: 0.3211561621838619

Step: 900
loss: 0.0018
grad_norm: 186.92938232421875
learning_rate: 0.0017591328783621036
epoch: 0.36130068245684466

Step: 1000
loss: 0.0019
grad_norm: 256.7528991699219
learning_rate: 0.0017323698648467817
epoch: 0.40144520272982737

Step: 1100
loss: 0.0018
grad_norm: 215.1677703857422
learning_rate: 0.0017056068513314599
epoch: 0.44158972300281013

Step: 1200
loss: 0.0019
grad_norm: 163.8228302001953
learning_rate: 0.001678843837816138
epoch: 0.48173424327579284

Step: 1300
loss: 0.0018
grad_norm: 687.027587890625
learning_rate: 0.0016520808243008164
epoch: 0.5218787635487756

Step: 1400
loss: 0.0017
grad_norm: 429.1647644042969
learning_rate: 0.0016253178107854946
epoch: 0.5620232838217584

Step: 1500
loss: 0.0018
grad_norm: 203.08607482910156
learning_rate: 0.0015985547972701727
epoch: 0.602167804094741

Step: 1600
loss: 0.0018
grad_norm: 289.3648986816406
learning_rate: 0.0015717917837548508
epoch: 0.6423123243677238

Step: 1700
loss: 0.0018
grad_norm: 228.7225341796875
learning_rate: 0.001545028770239529
epoch: 0.6824568446407066

Step: 1800
loss: 0.0017
grad_norm: 340.1418762207031
learning_rate: 0.0015182657567242071
epoch: 0.7226013649136893

Step: 1900
loss: 0.0017
grad_norm: 319.7049560546875
learning_rate: 0.0014915027432088855
epoch: 0.762745885186672

Step: 2000
loss: 0.0017
grad_norm: 227.51641845703125
learning_rate: 0.0014647397296935637
epoch: 0.8028904054596547

Step: 2100
loss: 0.0017
grad_norm: 163.5001983642578
learning_rate: 0.0014379767161782418
epoch: 0.8430349257326375

Step: 2200
loss: 0.0017
grad_norm: 212.3007049560547
learning_rate: 0.00141121370266292
epoch: 0.8831794460056203

Step: 2300
loss: 0.0016
grad_norm: 128.26531982421875
learning_rate: 0.001384450689147598
epoch: 0.923323966278603

Step: 2400
loss: 0.0017
grad_norm: 188.68081665039062
learning_rate: 0.0013576876756322763
epoch: 0.9634684865515857

Step: 2491
eval_loss: 0.001724576111882925
eval_runtime: 71.8224
eval_samples_per_second: 58.422
eval_steps_per_second: 1.838
epoch: 1.0

Step: 2500
loss: 0.0017
grad_norm: 232.69390869140625
learning_rate: 0.0013309246621169544
epoch: 1.0036130068245686

Step: 2600
loss: 0.0016
grad_norm: 134.9673614501953
learning_rate: 0.0013041616486016326
epoch: 1.0437575270975512

Step: 2700
loss: 0.0016
grad_norm: 158.98594665527344
learning_rate: 0.0012773986350863107
epoch: 1.0839020473705339

Step: 2800
loss: 0.0016
grad_norm: 164.5272674560547
learning_rate: 0.0012506356215709888
epoch: 1.1240465676435167

Step: 2900
loss: 0.0016
grad_norm: 215.5190887451172
learning_rate: 0.001223872608055667
epoch: 1.1641910879164994

Step: 3000
loss: 0.0016
grad_norm: 149.02622985839844
learning_rate: 0.0011971095945403451
epoch: 1.204335608189482

Step: 3100
loss: 0.0016
grad_norm: 130.45361328125
learning_rate: 0.0011703465810250233
epoch: 1.244480128462465

Step: 3200
loss: 0.0016
grad_norm: 151.4455108642578
learning_rate: 0.0011435835675097017
epoch: 1.2846246487354476

Step: 3300
loss: 0.0016
grad_norm: 118.28164672851562
learning_rate: 0.0011168205539943798
epoch: 1.3247691690084302

Step: 3400
loss: 0.0016
grad_norm: 110.20338439941406
learning_rate: 0.001090057540479058
epoch: 1.364913689281413

Step: 3500
loss: 0.0016
grad_norm: 183.21633911132812
learning_rate: 0.001063294526963736
epoch: 1.4050582095543958

Step: 3600
loss: 0.0016
grad_norm: 114.47438049316406
learning_rate: 0.0010365315134484143
epoch: 1.4452027298273786

Step: 3700
loss: 0.0015
grad_norm: 122.34828186035156
learning_rate: 0.0010097684999330924
epoch: 1.4853472501003613

Step: 3800
loss: 0.0015
grad_norm: 137.05743408203125
learning_rate: 0.0009830054864177705
epoch: 1.525491770373344

Step: 3900
loss: 0.0016
grad_norm: 125.3991470336914
learning_rate: 0.0009562424729024488
epoch: 1.5656362906463268

Step: 4000
loss: 0.0016
grad_norm: 112.5672836303711
learning_rate: 0.000929479459387127
epoch: 1.6057808109193095

Step: 4100
loss: 0.0015
grad_norm: 148.339599609375
learning_rate: 0.0009027164458718052
epoch: 1.6459253311922923

Step: 4200
loss: 0.0016
grad_norm: 120.0624008178711
learning_rate: 0.0008759534323564834
epoch: 1.686069851465275

Step: 4300
loss: 0.0016
grad_norm: 154.708740234375
learning_rate: 0.0008491904188411615
epoch: 1.7262143717382576

Step: 4400
loss: 0.0015
grad_norm: 133.44583129882812
learning_rate: 0.0008224274053258397
epoch: 1.7663588920112403

Step: 4500
loss: 0.0015
grad_norm: 115.60010528564453
learning_rate: 0.0007956643918105179
epoch: 1.8065034122842232

Step: 4600
loss: 0.0016
grad_norm: 130.53985595703125
learning_rate: 0.0007689013782951961
epoch: 1.846647932557206

Step: 4700
loss: 0.0016
grad_norm: 223.8297882080078
learning_rate: 0.0007421383647798742
epoch: 1.8867924528301887

Step: 4800
loss: 0.0015
grad_norm: 112.81388854980469
learning_rate: 0.0007153753512645525
epoch: 1.9269369731031714

Step: 4900
loss: 0.0015
grad_norm: 114.86338806152344
learning_rate: 0.0006886123377492306
epoch: 1.967081493376154

Step: 4982
eval_loss: 0.0015312688192352653
eval_runtime: 83.4022
eval_samples_per_second: 50.31
eval_steps_per_second: 1.583
epoch: 2.0

Step: 5000
loss: 0.0015
grad_norm: 112.48574829101562
learning_rate: 0.0006618493242339088
epoch: 2.007226013649137

Step: 5100
loss: 0.0015
grad_norm: 182.1645965576172
learning_rate: 0.0006350863107185869
epoch: 2.0473705339221198

Step: 5200
loss: 0.0015
grad_norm: 118.1332778930664
learning_rate: 0.0006083232972032651
epoch: 2.0875150541951024

Step: 5300
loss: 0.0014
grad_norm: 142.1005096435547
learning_rate: 0.0005815602836879432
epoch: 2.127659574468085

Step: 5400
loss: 0.0015
grad_norm: 160.08633422851562
learning_rate: 0.0005547972701726215
epoch: 2.1678040947410677

Step: 5500
loss: 0.0015
grad_norm: 94.79603576660156
learning_rate: 0.0005280342566572996
epoch: 2.2079486150140504

Step: 5600
loss: 0.0014
grad_norm: 164.45391845703125
learning_rate: 0.0005012712431419778
epoch: 2.2480931352870335

Step: 5700
loss: 0.0014
grad_norm: 145.71644592285156
learning_rate: 0.00047450822962665596
epoch: 2.288237655560016

Step: 5800
loss: 0.0014
grad_norm: 128.37010192871094
learning_rate: 0.0004477452161113341
epoch: 2.328382175832999

Step: 5900
loss: 0.0015
grad_norm: 127.83977508544922
learning_rate: 0.0004209822025960123
epoch: 2.3685266961059814

Step: 6000
loss: 0.0014
grad_norm: 180.08908081054688
learning_rate: 0.00039421918908069046
epoch: 2.408671216378964

Step: 6100
loss: 0.0014
grad_norm: 136.62869262695312
learning_rate: 0.00036745617556536867
epoch: 2.448815736651947

Step: 6200
loss: 0.0014
grad_norm: 89.96891021728516
learning_rate: 0.00034069316205004687
epoch: 2.48896025692493

Step: 6300
loss: 0.0014
grad_norm: 137.79910278320312
learning_rate: 0.000313930148534725
epoch: 2.5291047771979125

Step: 6400
loss: 0.0015
grad_norm: 127.57261657714844
learning_rate: 0.00028716713501940317
epoch: 2.569249297470895

Step: 6500
loss: 0.0014
grad_norm: 162.61671447753906
learning_rate: 0.00026040412150408137
epoch: 2.609393817743878

Step: 6600
loss: 0.0014
grad_norm: 108.19882202148438
learning_rate: 0.00023364110798875954
epoch: 2.6495383380168605

Step: 6700
loss: 0.0015
grad_norm: 143.34945678710938
learning_rate: 0.00020687809447343772
epoch: 2.6896828582898435

Step: 6800
loss: 0.0014
grad_norm: 142.79087829589844
learning_rate: 0.00018011508095811587
epoch: 2.729827378562826

Step: 6900
loss: 0.0014
grad_norm: 107.69197845458984
learning_rate: 0.00015335206744279407
epoch: 2.769971898835809

Step: 7000
loss: 0.0014
grad_norm: 120.61671447753906
learning_rate: 0.00012658905392747225
epoch: 2.8101164191087915

Step: 7100
loss: 0.0014
grad_norm: 165.38815307617188
learning_rate: 9.982604041215041e-05
epoch: 2.8502609393817746

Step: 7200
loss: 0.0014
grad_norm: 131.31605529785156
learning_rate: 7.306302689682858e-05
epoch: 2.8904054596547573

Step: 7300
loss: 0.0014
grad_norm: 121.56271362304688
learning_rate: 4.630001338150676e-05
epoch: 2.93054997992774

Step: 7400
loss: 0.0014
grad_norm: 128.50767517089844
learning_rate: 1.953699986618493e-05
epoch: 2.9706945002007226

Step: 7473
eval_loss: 0.0014654488768428564
eval_runtime: 85.2083
eval_samples_per_second: 49.244
eval_steps_per_second: 1.549
epoch: 3.0

Step: 7473
train_runtime: 10050.8313
train_samples_per_second: 23.793
train_steps_per_second: 0.744
total_flos: 8.890622346579149e+16
train_loss: 0.017235732433085425
epoch: 3.0

